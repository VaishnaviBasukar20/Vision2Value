{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13334140,"sourceType":"datasetVersion","datasetId":8454507}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers==4.57.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM, AutoFeatureExtractor\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport torch.optim as optim\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport os\nimport hashlib\nfrom datetime import datetime","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil, os\n\n# # Be absolutely sure before running this\n# for item in os.listdir(\"/kaggle/working\"):\n#     item_path = os.path.join(\"/kaggle/working\", item)\n#     try:\n#         if os.path.isfile(item_path) or os.path.islink(item_path):\n#             os.unlink(item_path)  # remove file or symlink\n#         elif os.path.isdir(item_path):\n#             shutil.rmtree(item_path)  # remove folder\n#     except Exception as e:\n#         print(f\"Failed to delete {item_path}: {e}\")\n\n# print(\"✅ /kaggle/working has been emptied.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(r\"/kaggle/input/amazon-ml-challenge-2025-dataset/train.csv\")\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_value_unit(text):\n    value_match = re.search(r\"Value:\\s*([\\d.]+)\", text)\n    unit_match = re.search(r\"Unit:\\s*([a-zA-Z]+)\", text)\n    \n    value = float(value_match.group(1)) if value_match else None\n    unit = unit_match.group(1) if unit_match else None\n    return pd.Series([value, unit])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_text(text):\n    if pd.isna(text):\n        return text\n    text = str(text)\n\n    # Remove unwanted prefixes\n    text = re.sub(r\"(?i)\\bItem Name:\\s*\", \"\", text)\n    text = re.sub(r\"(?i)\\bBullet Point\\s*\\d*:\\s*\", \"\", text)\n    text = re.sub(r\"(?i)\\bProduct Description:\\s*\", \"\", text)\n    text = re.sub(r\"(?i)\\bValue:\\s*[\\d.]+\\b\", \"\", text)\n    text = re.sub(r\"(?i)\\bUnit:\\s*[a-zA-Z]+\\b\", \"\", text)\n\n    # Remove HTML tags\n    text = re.sub(r\"<[^>]+>\", \"\", text)\n\n    # Remove extra spaces or newlines\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[['Value', 'Unit']] = df['catalog_content'].apply(extract_value_unit)\ndf[\"catalog_content\"] = df[\"catalog_content\"].apply(clean_text)\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.Unit.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.Unit.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Unit'].mode()[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Value'].median()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.Value.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.Value.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Unit'] = df['Unit'].fillna('None')\ndf['Value'] = df['Value'].fillna(df['Value'].median())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"le = LabelEncoder()\ndf['Unit'] = le.fit_transform(df['Unit'])\n\nunit_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(\"Unit mapping:\", unit_mapping)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\ndf['Value'] = scaler.fit_transform(df[['Value']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\n# Raw price histogram\nplt.subplot(1, 2, 1)\nsns.histplot(df.price, bins=60, color='teal', alpha=0.7, edgecolor='black')\nplt.title(\"Original Price Distribution\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Frequency\")\n\n# Boxplot to detect outliers\nplt.subplot(1, 2, 2)\nsns.boxplot(x=df.price, color='orange')\nplt.title(\"Boxplot of Prices\")\nplt.xlabel(\"Price\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(df['price'], bins=50, kde=True, color='skyblue')\nplt.title('Price Distribution', fontsize=16)\nplt.xlabel('Price', fontsize=14)\nplt.ylabel('Frequency', fontsize=14)\nplt.grid(alpha=0.3)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(np.log1p(df['price']), bins=50, kde=True, color='orange')\nplt.title('Log-Scaled Price Distribution', fontsize=16)\nplt.xlabel('Log(Price + 1)', fontsize=14)\nplt.ylabel('Frequency', fontsize=14)\nplt.grid(alpha=0.3)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Q1 = np.percentile(df.price, 25)\nQ3 = np.percentile(df.price, 75)\nIQR = Q3 - Q1\n\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\nprint(f\"Price range (IQR-based): {lower_limit:.2f} to {upper_limit:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lower_percentile = 1\nupper_percentile = 99\nlower_clip = np.percentile(df.price, lower_percentile)\nupper_clip = np.percentile(df.price, upper_percentile)\n\nprint(f\"Clipping range (1st–99th percentile): {lower_clip:.2f} to {upper_clip:.2f}\")\nprint(df.price.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clipped_prices = np.clip(df.price, lower_clip, upper_clip)\n\nplt.figure(figsize=(10, 5))\nsns.histplot(np.log1p(clipped_prices), bins=50, kde=True, color='coral', alpha=0.7)\nplt.title(\"Log-Scaled Price Distribution (After Clipping)\")\nplt.xlabel(\"log(Price + 1)\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lower_limit =  1.32\nupper_limit = 145.25\n\n# Keep only rows within the desired range\nfiltered_df = df[(df[\"price\"] >= lower_limit) & (df[\"price\"] <= upper_limit)].copy()\n\nprint(f\"Original dataset size: {len(df)}\")\nprint(f\"After filtering: {len(filtered_df)}\")\nprint(f\"Rows removed: {len(df) - len(filtered_df)} ({(1 - len(filtered_df)/len(df))*100:.2f}% removed)\")\nprint(f'Total rows under lower bound: {len(df[(df[\"price\"] < lower_limit)])}')\nprint(f'Total rows above upper bound: {len(df[(df[\"price\"] > upper_limit)])}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.histplot(np.log1p(filtered_df[\"price\"]), bins=50, kde=True, color='green')\nplt.title(\"Log(Price + 1) Distribution After Outlier Removal\")\nplt.xlabel(\"log(Price + 1)\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[(df[\"price\"] >= lower_limit) & (df[\"price\"] <= upper_limit)].copy()\nactual_price = df.price\ndf[\"log_price\"] = np.log1p(df[\"price\"])\ndf.drop(columns=[\"price\",\"sample_id\"], inplace=True)\ntotal_rows = len(df)\ntrain_rows = int(total_rows * 0.8)\nval_rows = total_rows - train_rows\n\nprint(f\"Final number of training rows: {train_rows}\")\nprint(f\"Final number of validation rows: {val_rows}\")\nprint(f\"Final number of total rows: {total_rows}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(df['log_price'], bins=50, kde=True, color='orange')\nplt.title('Log-Scaled Price Distribution', fontsize=16)\nplt.xlabel('Log(Price + 1)', fontsize=14)\nplt.ylabel('Frequency', fontsize=14)\nplt.grid(alpha=0.3)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_ = df.sample(n=15000, random_state=42).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:11.630878Z","iopub.execute_input":"2025-10-13T06:22:11.631155Z","iopub.status.idle":"2025-10-13T06:22:11.643903Z","shell.execute_reply.started":"2025-10-13T06:22:11.631135Z","shell.execute_reply":"2025-10-13T06:22:11.643226Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(df_['log_price'], bins=50, kde=True, color='orange')\nplt.title('Log-Scaled Price Distribution', fontsize=16)\nplt.xlabel('Log(Price + 1)', fontsize=14)\nplt.ylabel('Frequency', fontsize=14)\nplt.grid(alpha=0.3)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\ntoken_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in df['catalog_content']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:14.521728Z","iopub.execute_input":"2025-10-13T06:22:14.522402Z","iopub.status.idle":"2025-10-13T06:22:24.069756Z","shell.execute_reply.started":"2025-10-13T06:22:14.522376Z","shell.execute_reply":"2025-10-13T06:22:24.069171Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a145f8a657f49a68b2be26a87928ecb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a043909880447696611bcd2d22eb9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b8aa16a2d274101912aaf4f4eb08412"}},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"# Convert to NumPy array for convenience\ntoken_lengths = np.array(token_lengths)\n\n# Basic statistics\nprint(\"Min tokens:\", token_lengths.min())\nprint(\"Max tokens:\", token_lengths.max())\nprint(\"Median tokens:\", np.median(token_lengths))\nprint(\"Mean tokens:\", np.mean(token_lengths))\nprint(\"90th percentile:\", np.percentile(token_lengths, 90))\nprint(\"95th percentile:\", np.percentile(token_lengths, 95))\nprint(\"99th percentile:\", np.percentile(token_lengths, 99))\n\n# Histogram\nplt.figure(figsize=(10,5))\nplt.hist(token_lengths, bins=50, color='skyblue', edgecolor='black')\nplt.title(\"Token length distribution\")\nplt.xlabel(\"Number of tokens\")\nplt.ylabel(\"Number of texts\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:24.070891Z","iopub.execute_input":"2025-10-13T06:22:24.071235Z","iopub.status.idle":"2025-10-13T06:22:24.266565Z","shell.execute_reply.started":"2025-10-13T06:22:24.071216Z","shell.execute_reply":"2025-10-13T06:22:24.265824Z"}},"outputs":[{"name":"stdout","text":"Min tokens: 4\nMax tokens: 1697\nMedian tokens: 137.0\nMean tokens: 200.34546666666665\n90th percentile: 509.0\n95th percentile: 586.0499999999993\n99th percentile: 834.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"max_len_optimal = 196\n\ndef batch_tokenize(texts, batch_size=256, max_len=512):\n    all_input_ids, all_attention_masks = [], []\n    for i in range(0, len(texts), batch_size):\n        enc = tokenizer(\n            texts[i:i+batch_size].tolist(),\n            padding='max_length',\n            truncation=True,\n            max_length=max_len,\n            return_tensors='pt'\n        )\n        all_input_ids.append(enc['input_ids'])\n        all_attention_masks.append(enc['attention_mask'])\n    return torch.cat(all_input_ids, dim=0), torch.cat(all_attention_masks, dim=0)\n\ninput_ids_all, attention_mask_all = batch_tokenize(df['catalog_content'], batch_size=256, max_len=max_len_optimal)\n\nprices_all = torch.from_numpy(df['log_price'].to_numpy()).float()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:24.267357Z","iopub.execute_input":"2025-10-13T06:22:24.267554Z","iopub.status.idle":"2025-10-13T06:22:33.311481Z","shell.execute_reply.started":"2025-10-13T06:22:24.267539Z","shell.execute_reply":"2025-10-13T06:22:33.310820Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"train_idx, val_idx = train_test_split(range(len(df)), test_size=0.2, random_state=42)\n\ntrain_idx_torch = torch.tensor(train_idx, dtype=torch.long)\nval_idx_torch = torch.tensor(val_idx, dtype=torch.long)\n\ntrain_input_ids = input_ids_all[train_idx_torch]\ntrain_attention_mask = attention_mask_all[train_idx_torch]\ntrain_prices = prices_all[train_idx_torch]\n\nval_input_ids = input_ids_all[val_idx_torch]\nval_attention_mask = attention_mask_all[val_idx_torch]\nval_prices = prices_all[val_idx_torch]\n\ntrain_urls = df['image_link'].iloc[train_idx].to_numpy()\nval_urls = df['image_link'].iloc[val_idx].to_numpy()\n\ntrain_numeric = torch.tensor(df[['Value', 'Unit']].iloc[train_idx].values, dtype=torch.float32)\nval_numeric = torch.tensor(df[['Value', 'Unit']].iloc[val_idx].values, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:33.313129Z","iopub.execute_input":"2025-10-13T06:22:33.313434Z","iopub.status.idle":"2025-10-13T06:22:33.349728Z","shell.execute_reply.started":"2025-10-13T06:22:33.313416Z","shell.execute_reply":"2025-10-13T06:22:33.349195Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"CACHE_DIR = \"/kaggle/working/image_cache\"\nLOG_FILE = os.path.join(CACHE_DIR, \"missing_images_log.txt\")\nos.makedirs(CACHE_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:32:00.715064Z","iopub.execute_input":"2025-10-13T06:32:00.715342Z","iopub.status.idle":"2025-10-13T06:32:00.719271Z","shell.execute_reply.started":"2025-10-13T06:32:00.715321Z","shell.execute_reply":"2025-10-13T06:32:00.718654Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"def load_image_cached(url, type_, cache_dir=CACHE_DIR):\n    \"\"\"\n    Downloads an image from a URL once, caches it as JPEG, \n    and returns a PyTorch tensor ready for ViT.\n    \"\"\"\n    # Generate unique filename from URL\n    filename = hashlib.md5(url.encode()).hexdigest() + \".jpg\"\n    os.makedirs(f\"{CACHE_DIR}/{type_}\", exist_ok=True)\n    filepath = os.path.join(cache_dir, type_, filename)\n    \n    if os.path.exists(filepath):\n        # Load cached image\n        img = Image.open(filepath).convert(\"RGB\")\n    else:\n        # Download image\n        try:\n            response = requests.get(url, timeout=10)\n            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n            img.save(filepath, format='JPEG', quality=85)\n        except Exception as e:\n            with open(LOG_FILE, \"a\") as f:\n                f.write(f\"[{datetime.now()}] Failed to load: {url} | Error: {e}\\n\")\n                \n            img = Image.new(\"RGB\", (224,224), color=(0,0,0))\n    \n    return img\n\nclass MultimodalDataset(Dataset):\n    def __init__(self, input_ids, attention_mask, urls, numeric_features, prices, type_, image_transform=None):\n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n        self.urls = urls\n        self.numeric_features = numeric_features\n        self.prices = prices\n        self.image_transform = image_transform\n        self.type_ = type_\n\n    def __len__(self):\n        return len(self.urls)\n\n    def __getitem__(self, idx):\n        input_id = self.input_ids[idx]\n        attention_mask = self.attention_mask[idx]\n\n        image = load_image_cached(self.urls[idx], type_=self.type_)\n        if self.image_transform:\n            image = self.image_transform(image)\n\n        numeric = self.numeric_features[idx]\n\n        # print(type(self.prices[idx]))\n        price = torch.tensor(self.prices[idx], dtype=torch.float32)\n\n        return {\n            'input_ids': input_id,\n            'attention_mask': attention_mask,\n            'pixel_values': image,\n            'numeric_features': numeric,\n            'labels': price.unsqueeze(-1)\n        }\n\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                         std=[0.5, 0.5, 0.5])\n])\n\n# train_dataset = MultimodalDataset(\n#     input_ids=train_input_ids,\n#     attention_mask=train_attention_mask,\n#     urls=train_urls,\n#     numeric_features=train_numeric,\n#     prices=train_prices,\n#     type_=\"train\",\n#     image_transform=image_transform\n# )\n\n# val_dataset = MultimodalDataset(\n#     input_ids=val_input_ids,\n#     attention_mask=val_attention_mask,\n#     urls=val_urls,\n#     numeric_features=val_numeric,\n#     prices=val_prices,\n#     type_=\"val\",\n#     image_transform=image_transform\n# )\n\n# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:40:58.073828Z","iopub.execute_input":"2025-10-13T06:40:58.074589Z","iopub.status.idle":"2025-10-13T06:40:58.087043Z","shell.execute_reply.started":"2025-10-13T06:40:58.074562Z","shell.execute_reply":"2025-10-13T06:40:58.086123Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"def smape(y_true_log, y_pred_log):\n    y_true = torch.expm1(y_true_log)\n    y_pred = torch.expm1(y_pred_log)\n    numerator = torch.abs(y_pred - y_true)\n    denominator = (torch.abs(y_true) + torch.abs(y_pred)) / 2 + 1e-7\n    return torch.mean(numerator / denominator) * 100\n\ndef mae(y_true_log, y_pred_log):\n    y_true = torch.expm1(y_true_log)\n    y_pred = torch.expm1(y_pred_log)\n    return torch.mean(torch.abs(y_pred - y_true))\n\ndef smape_loss(y_true, y_pred, eps=1e-7):\n    numerator = torch.abs(y_pred - y_true)\n    denominator = (torch.abs(y_true) + torch.abs(y_pred)) / 2 + eps\n    return torch.mean(numerator / denominator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:33.373380Z","iopub.execute_input":"2025-10-13T06:22:33.373685Z","iopub.status.idle":"2025-10-13T06:22:33.385721Z","shell.execute_reply.started":"2025-10-13T06:22:33.373660Z","shell.execute_reply":"2025-10-13T06:22:33.384980Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"bert_model = AutoModelForMaskedLM.from_pretrained(\"answerdotai/ModernBERT-base\")\nvit_model = AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n\n# Freeze most layers (optional for faster training)\nfor param in bert_model.parameters():\n    param.requires_grad = False\nfor param in vit_model.parameters():\n    param.requires_grad = False\n\nclass CrossAttentionFusion(nn.Module):\n    def __init__(self, hidden_dim=768, num_heads=4):\n        super().__init__()\n        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n        self.norm = nn.LayerNorm(hidden_dim)\n\n    def forward(self, text_emb, image_emb):\n        # Query = text, Key/Value = image\n        attn_output, _ = self.cross_attention(query=text_emb, key=image_emb, value=image_emb)\n        out = self.norm(text_emb + attn_output)\n        return out\n\nclass MultimodalRegressor(nn.Module):\n    def __init__(self, bert_model, vit_model, numeric_dim=2, hidden_dim=768):\n        super().__init__()\n        self.bert = bert_model.base_model  # encoder part\n        self.vit = vit_model\n        self.fusion = CrossAttentionFusion(hidden_dim=hidden_dim)\n        \n        # Include numeric features\n        self.numeric_proj = nn.Sequential(\n            nn.Linear(numeric_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU()\n        )\n\n        self.regressor = nn.Sequential(\n            nn.Linear(hidden_dim + 64, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, input_ids, attention_mask, pixel_values, numeric_features):\n        # Text embeddings\n        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_emb = text_outputs.last_hidden_state  # [B, seq_len, 768]\n\n        # Image embeddings\n        image_outputs = self.vit(pixel_values)\n        image_emb = image_outputs.last_hidden_state  # [B, num_patches, 768]\n\n        # Cross Attention Fusion\n        fused = self.fusion(text_emb, image_emb)  # [B, seq_len, 768]\n\n        # Mean pooling\n        pooled = fused.mean(dim=1)  # [B, 768]\n\n        # Numeric features projection\n        numeric_proj = self.numeric_proj(numeric_features)  # [B, 64]\n\n        # Concatenate\n        combined = torch.cat([pooled, numeric_proj], dim=1)  # [B, 768+64]\n\n        # Regression output\n        price = self.regressor(combined)\n        return price\n        \nmodel = MultimodalRegressor(bert_model, vit_model)\nprint(sum(p.numel() for p in model.parameters() if p.requires_grad), \"trainable parameters\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:33.387395Z","iopub.execute_input":"2025-10-13T06:22:33.387617Z","iopub.status.idle":"2025-10-13T06:22:54.072260Z","shell.execute_reply.started":"2025-10-13T06:22:33.387602Z","shell.execute_reply":"2025-10-13T06:22:54.071473Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"812d2bbf867f464698feea1ad8ca06bd"}},"metadata":{}},{"name":"stderr","text":"2025-10-13 06:22:34.996415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760336555.176779      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760336555.223938      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6898cec30ce748beaa1d0ea43d4ae4cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea6da4d2e1824f3bb48c9bc16adb894b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38f3c3a8b2b8447a8a5f64ce8bf50a12"}},"metadata":{}},{"name":"stdout","text":"2860545 trainable parameters\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"# class SimpleMultimodalRegressor(nn.Module):\n#     def __init__(self, bert_model, vit_model, numeric_dim=2, hidden_dim=256):\n#         super().__init__()\n#         # Use pretrained backbones\n#         self.bert = bert_model.base_model  # encoder part\n#         self.vit = vit_model\n\n#         # Freeze most layers for efficiency\n#         for param in self.bert.parameters():\n#             param.requires_grad = False\n#         for param in self.vit.parameters():\n#             param.requires_grad = False\n\n#         # Numeric features projection\n#         self.numeric_proj = nn.Sequential(\n#             nn.Linear(numeric_dim, 32),\n#             nn.ReLU()\n#         )\n\n#         # Final regressor MLP\n#         # text_dim + image_dim + numeric_proj_dim\n#         combined_dim = 768 + 768 + 32\n#         self.regressor = nn.Sequential(\n#             nn.Linear(combined_dim, hidden_dim),\n#             nn.ReLU(),\n#             nn.Dropout(0.2),\n#             nn.Linear(hidden_dim, 1)\n#         )\n\n#     def forward(self, input_ids, attention_mask, pixel_values, numeric_features):\n#         # Text embedding: use CLS token\n#         text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n#         text_emb = text_outputs.last_hidden_state[:, 0, :]  # [B, 768]\n\n#         # Image embedding: mean pooling over patches\n#         image_outputs = self.vit(pixel_values)\n#         image_emb = image_outputs.last_hidden_state.mean(dim=1)  # [B, 768]\n\n#         # Numeric features\n#         numeric_proj = self.numeric_proj(numeric_features)  # [B, 32]\n\n#         # Concatenate\n#         combined = torch.cat([text_emb, image_emb, numeric_proj], dim=1)  # [B, 1568]\n\n#         # Regression output\n#         price = self.regressor(combined)\n#         return price\n\n# # Instantiate\n# model = SimpleMultimodalRegressor(bert_model, vit_model)\n# print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:54.073737Z","iopub.execute_input":"2025-10-13T06:22:54.074249Z","iopub.status.idle":"2025-10-13T06:22:54.078524Z","shell.execute_reply.started":"2025-10-13T06:22:54.074230Z","shell.execute_reply":"2025-10-13T06:22:54.077839Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"class ModelCheckpoint:\n    \"\"\"\n    Saves the best model based on a monitored metric.\n    \"\"\"\n    def __init__(self, filepath, monitor='val_loss', mode='min', verbose=True):\n        self.filepath = filepath\n        self.monitor = monitor\n        self.mode = mode\n        self.verbose = verbose\n        if mode == 'min':\n            self.best = float('inf')\n        elif mode == 'max':\n            self.best = -float('inf')\n        else:\n            raise ValueError(\"mode must be 'min' or 'max'\")\n\n    def step(self, metric_value, model):\n        \"\"\"\n        Call this after each validation step.\n        \"\"\"\n        is_improved = False\n        if self.mode == 'min' and metric_value < self.best:\n            is_improved = True\n        elif self.mode == 'max' and metric_value > self.best:\n            is_improved = True\n\n        if is_improved:\n            if self.verbose:\n                print(f\"Metric improved ({self.best:.6f} -> {metric_value:.6f}). Saving model to {self.filepath}\")\n            self.best = metric_value\n            torch.save(model.state_dict(), self.filepath)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:54.079210Z","iopub.execute_input":"2025-10-13T06:22:54.079439Z","iopub.status.idle":"2025-10-13T06:22:54.116503Z","shell.execute_reply.started":"2025-10-13T06:22:54.079422Z","shell.execute_reply":"2025-10-13T06:22:54.115750Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    scaler = torch.cuda.amp.GradScaler()\n\n    for batch in tqdm(dataloader, desc=\"Training\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        numeric_features = batch['numeric_features'].to(device)\n        labels = batch['labels'].to(device).view(-1, 1)\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n            outputs = model(input_ids, attention_mask, pixel_values, numeric_features)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item() * labels.size(0)\n\n    return running_loss / len(dataloader.dataset)\n    \n\ndef evaluate(model, dataloader, device):\n    model.eval()\n    all_preds, all_targets = [], []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            numeric_features = batch['numeric_features'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask, pixel_values, numeric_features)\n            all_preds.append(outputs)\n            all_targets.append(labels)\n\n    all_preds = torch.cat(all_preds, dim=0)\n    all_targets = torch.cat(all_targets, dim=0)\n\n    return smape(all_targets, all_preds).item(), mae(all_targets, all_preds).item()\n\ndef train_model(model, train_loader, val_loader, epochs=10, lr=1e-4, device='cuda'):\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = smape_loss # nn.HuberLoss(delta=1.0)\n    \n    history = {\n        'train_loss': [],\n        'val_smape': [],\n        'val_mae': []\n    }\n\n    checkpoint_cb = ModelCheckpoint(\"best_model.pth\", mode=\"min\")\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n        val_smape, val_mae = evaluate(model, val_loader, device)\n        \n        # Save metrics\n        history['train_loss'].append(train_loss)\n        history['val_smape'].append(val_smape)\n        history['val_mae'].append(val_mae)\n        \n        print(f\"Train Loss: {train_loss:.4f} | Val SMAPE: {val_smape:.4f} | Val MAE: {val_mae:.4f}\")\n        checkpoint_cb.step(val_smape, model)\n\n    print(\"Training complete!\")\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:54.118065Z","iopub.execute_input":"2025-10-13T06:22:54.118471Z","iopub.status.idle":"2025-10-13T06:22:54.130845Z","shell.execute_reply.started":"2025-10-13T06:22:54.118440Z","shell.execute_reply":"2025-10-13T06:22:54.130176Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"def plot_training_curves(history):\n    epochs = range(1, len(history['train_loss'])+1)\n    \n    plt.figure(figsize=(16,5))\n    \n    # Training Loss\n    plt.subplot(1,2,1)\n    plt.plot(epochs, history['train_loss'], label='Train Loss (Huber)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training Loss Curve')\n    plt.grid(True)\n    plt.legend()\n\n    # Validation Metrics\n    plt.subplot(1,2,2)\n    plt.plot(epochs, history['val_smape'], label='Val SMAPE')\n    plt.plot(epochs, history['val_mae'], label='Val MAE')\n    plt.xlabel('Epoch')\n    plt.ylabel('Metric')\n    plt.title('Validation Metrics')\n    plt.grid(True)\n    plt.legend()\n    \n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:54.131475Z","iopub.execute_input":"2025-10-13T06:22:54.131716Z","iopub.status.idle":"2025-10-13T06:22:54.143500Z","shell.execute_reply.started":"2025-10-13T06:22:54.131666Z","shell.execute_reply":"2025-10-13T06:22:54.142817Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"def predict_price(model, tokenizer, texts, images, device='cuda', max_len=512):\n    \"\"\"\n    texts: list of strings (catalog_content)\n    images: torch.Tensor of shape [B, 3, H, W] (preprocessed)\n    \"\"\"\n    model.eval()\n    \n    # Tokenize text\n    enc = tokenizer(\n        texts,\n        padding='max_length',\n        truncation=True,\n        max_length=max_len,\n        return_tensors='pt'\n    )\n    input_ids = enc['input_ids'].to(device)\n    attention_mask = enc['attention_mask'].to(device)\n    pixel_values = images.to(device)\n    \n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask, pixel_values)  # [B, 1]\n        log_prices = outputs.squeeze(-1)  # [B]\n        prices = torch.expm1(log_prices)  # convert back to real prices\n    \n    return prices.cpu().numpy()  # return as NumPy array","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:22:54.144261Z","iopub.execute_input":"2025-10-13T06:22:54.144996Z","iopub.status.idle":"2025-10-13T06:22:54.156356Z","shell.execute_reply.started":"2025-10-13T06:22:54.144972Z","shell.execute_reply":"2025-10-13T06:22:54.155608Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"# train_dataset = MultimodalDataset(train_input_ids, train_attention_mask, train_urls, train_prices, \"train\", image_transform)\n# val_dataset = MultimodalDataset(val_input_ids, val_attention_mask, val_urls, val_prices, \"val\", image_transform)\n\ntrain_dataset = MultimodalDataset(\n    input_ids=train_input_ids,\n    attention_mask=train_attention_mask,\n    urls=train_urls,\n    numeric_features=train_numeric,\n    prices=train_prices,\n    type_=\"train\",\n    image_transform=image_transform\n)\n\nval_dataset = MultimodalDataset(\n    input_ids=val_input_ids,\n    attention_mask=val_attention_mask,\n    urls=val_urls,\n    numeric_features=val_numeric,\n    prices=val_prices,\n    type_=\"val\",\n    image_transform=image_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n\nmodel = MultimodalRegressor(bert_model, vit_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:41:18.107344Z","iopub.execute_input":"2025-10-13T06:41:18.107927Z","iopub.status.idle":"2025-10-13T06:41:18.135840Z","shell.execute_reply.started":"2025-10-13T06:41:18.107889Z","shell.execute_reply":"2025-10-13T06:41:18.135318Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"history = train_model(model, train_loader, val_loader, epochs=10, lr=1e-4, device='cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T06:41:20.167914Z","iopub.execute_input":"2025-10-13T06:41:20.168598Z","iopub.status.idle":"2025-10-13T07:11:51.947598Z","shell.execute_reply.started":"2025-10-13T06:41:20.168573Z","shell.execute_reply":"2025-10-13T07:11:51.945917Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/2684261995.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/375 [00:00<?, ?it/s]/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\n/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\n/tmp/ipykernel_37/2684261995.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nTraining: 100%|██████████| 375/375 [16:06<00:00,  2.58s/it]\nEvaluating:   0%|          | 0/94 [00:00<?, ?it/s]/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\n/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\nEvaluating: 100%|██████████| 94/94 [04:21<00:00,  2.78s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3233 | Val SMAPE: 62.3402 | Val MAE: 13.2718\nMetric improved (inf -> 62.340237). Saving model to best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/2684261995.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/375 [00:00<?, ?it/s]/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\n/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\n/tmp/ipykernel_37/2684261995.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nTraining: 100%|██████████| 375/375 [03:56<00:00,  1.59it/s]\nEvaluating:   0%|          | 0/94 [00:00<?, ?it/s]/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\n/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\nEvaluating: 100%|██████████| 94/94 [01:31<00:00,  1.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2513 | Val SMAPE: 62.3171 | Val MAE: 13.3645\nMetric improved (62.340237 -> 62.317139). Saving model to best_model.pth\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/375 [00:00<?, ?it/s]/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\n/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\nTraining: 100%|██████████| 375/375 [03:55<00:00,  1.59it/s]\nEvaluating:   0%|          | 0/94 [00:00<?, ?it/s]/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\n/tmp/ipykernel_37/3927075259.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  price = torch.tensor(self.prices[idx], dtype=torch.float32)\nEvaluating:  38%|███▊      | 36/94 [00:36<00:59,  1.03s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/572189997.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/2684261995.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, lr, device)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mval_smape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Save metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/2684261995.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mall_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/1100945535.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, pixel_values, numeric_features)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Text embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtext_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mtext_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m  \u001b[0;31m# [B, seq_len, 768]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/modernbert/modeling_modernbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, sliding_window_mask, position_ids, inputs_embeds, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    867\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m             layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m    870\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/modernbert/modeling_modernbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, sliding_window_mask, position_ids, cu_seqlens, max_seqlen, output_attentions)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     ) -> torch.Tensor:\n\u001b[0;32m--> 542\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/modernbert/modeling_modernbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         attn_outputs = MODERNBERT_ATTENTION_FUNCTION[self.config._attn_implementation](\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mqkv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/modernbert/modeling_modernbert.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, qkv, attention_mask, sliding_window_mask, position_ids, local_attention, bs, dim, **_kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     attn_output = (\n\u001b[0;32m--> 416\u001b[0;31m         F.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":91},{"cell_type":"code","source":"plot_training_curves(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T20:30:39.470935Z","iopub.status.idle":"2025-10-12T20:30:39.471166Z","shell.execute_reply.started":"2025-10-12T20:30:39.471058Z","shell.execute_reply":"2025-10-12T20:30:39.471068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom torchvision import models, transforms\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport hashlib, os, requests\nfrom io import BytesIO\nimport numpy as np\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# -------- Text Embeddings --------\ntext_model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dim\ntext_model.eval()\n\ndef get_text_embeddings(texts, batch_size=64):\n    return text_model.encode(texts, convert_to_tensor=True, show_progress_bar=True, batch_size=batch_size)\n\ntrain_text_emb = get_text_embeddings(df['catalog_content'].iloc[train_idx].tolist())\nval_text_emb = get_text_embeddings(df['catalog_content'].iloc[val_idx].tolist())\n\n# -------- Image Embeddings --------\nimage_model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\nimage_model.heads = torch.nn.Identity()  # remove classification head\nimage_model.eval()\nimage_model.to(device)\n\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T07:11:57.089302Z","iopub.execute_input":"2025-10-13T07:11:57.089834Z","iopub.status.idle":"2025-10-13T07:12:37.141425Z","shell.execute_reply.started":"2025-10-13T07:11:57.089809Z","shell.execute_reply":"2025-10-13T07:12:37.140825Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a1c84c6c6a843f09f894814cf9ed3f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef89f8d2d0f43db8d073f9a79d59b17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"549601db0e3f4ab5819d524d1013f8af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adbdf572e29a4313a04bca10c9a0d8d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12998c76a95b426298c1c637f4d05b9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08c321e647064eaabf205acc6ba19fa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c82ab5e1dfd49bab84cfde9e748418d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b31b0ea334435ca600c12c250f1bf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a2447d13014299ab1191cb080a1ec8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"218b5703a08c48d39c4c5a7a82351844"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"838dd0d079a14582aac7df305897e741"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f6a8b2b8d6a473295c073dbb934df97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/47 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acff8b82ed9243c5ac23ee9b5a5d1690"}},"metadata":{}},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n100%|██████████| 330M/330M [00:01<00:00, 235MB/s] \n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"import os, hashlib, requests\nfrom io import BytesIO\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport torch\n\n# --- Caching + Loading ---\ndef load_image_cached(url, cache_dir):\n    \"\"\"Load image from local cache or download and save it.\"\"\"\n    os.makedirs(cache_dir, exist_ok=True)\n    filename = hashlib.md5(url.encode()).hexdigest() + \".jpg\"\n    filepath = os.path.join(cache_dir, filename)\n\n    if os.path.exists(filepath):\n        try:\n            img = Image.open(filepath).convert(\"RGB\")\n            return img\n        except Exception as e:\n            print(f\"[Warning] Corrupt cached image: {filepath} | {e}\")\n            img = Image.new(\"RGB\", (224, 224), color=(255, 255, 255))\n            img.save(filepath, format=\"JPEG\")\n            return img\n\n    try:\n        response = requests.get(url, timeout=10)\n        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n        img.save(filepath, format=\"JPEG\", quality=85)\n    except Exception as e:\n        print(f\"[Error] Failed to load {url}: {e}\")\n        img = Image.new(\"RGB\", (224, 224), color=(255, 255, 255))\n        img.save(filepath, format=\"JPEG\")\n    return img\n\n\n# --- Embedding Extraction ---\ndef get_image_embeddings(urls, cache_name, image_model, image_transform, device, cache_dir=\"image_cache\", batch_size=32):\n    \"\"\"\n    Generate embeddings using cached images (auto-resume and progress bar).\n    \"\"\"\n    cache_dir = os.path.join(cache_dir, cache_name)\n    os.makedirs(cache_dir, exist_ok=True)\n\n    embeddings = []\n\n    print(f\"🔹 Generating {cache_name} image embeddings...\")\n    for i in tqdm(range(0, len(urls), batch_size), desc=f\"{cache_name}\"):\n        batch_urls = urls[i:i + batch_size]\n        imgs = []\n\n        for url in batch_urls:\n            try:\n                img = image_transform(load_image_cached(url, cache_dir))\n                if img.shape != (3, 224, 224):\n                    img = torch.zeros(3, 224, 224)\n                imgs.append(img)\n            except Exception as e:\n                print(f\"[Warning] Skipping bad URL: {url} | {e}\")\n                imgs.append(torch.zeros(3, 224, 224))\n\n        imgs = torch.stack(imgs).to(device)\n\n        try:\n            with torch.no_grad():\n                emb = image_model(imgs)\n            embeddings.append(emb.cpu())\n        except Exception as e:\n            print(f\"[Error] Failed batch {i}: {e}\")\n            continue\n\n    embeddings = torch.cat(embeddings)\n    print(f\"✅ {cache_name} embeddings computed: {embeddings.shape}\")\n    return embeddings\n\ntrain_img_emb = get_image_embeddings(\n    train_urls,\n    cache_name=\"train\",\n    image_model=image_model,\n    image_transform=image_transform,\n    device=device,\n    batch_size=32\n)\n\nval_img_emb = get_image_embeddings(\n    val_urls,\n    cache_name=\"val\",\n    image_model=image_model,\n    image_transform=image_transform,\n    device=device,\n    batch_size=32\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T07:12:37.142552Z","iopub.execute_input":"2025-10-13T07:12:37.143153Z","iopub.status.idle":"2025-10-13T07:22:21.715920Z","shell.execute_reply.started":"2025-10-13T07:12:37.143132Z","shell.execute_reply":"2025-10-13T07:22:21.715051Z"}},"outputs":[{"name":"stdout","text":"🔹 Generating train image embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"train:   0%|          | 0/375 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11698c034e3347178913da6a3ea786f4"}},"metadata":{}},{"name":"stdout","text":"✅ train embeddings computed: torch.Size([12000, 768])\n🔹 Generating val image embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"val:   0%|          | 0/94 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56ca7fca948f4bec89f8634c2d4d7035"}},"metadata":{}},{"name":"stdout","text":"✅ val embeddings computed: torch.Size([3000, 768])\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"print(\"Text:\", val_text_emb.shape)\nprint(\"Image:\", val_img_emb.shape)\nprint(\"Numeric:\", val_numeric.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T07:32:13.356713Z","iopub.execute_input":"2025-10-13T07:32:13.356982Z","iopub.status.idle":"2025-10-13T07:32:13.361347Z","shell.execute_reply.started":"2025-10-13T07:32:13.356962Z","shell.execute_reply":"2025-10-13T07:32:13.360780Z"}},"outputs":[{"name":"stdout","text":"Text: torch.Size([3000, 384])\nImage: torch.Size([3000, 768])\nNumeric: torch.Size([3000, 2])\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"# -------- Numeric Features --------\ntrain_numeric = torch.tensor(df[['Value','Unit']].iloc[train_idx].values, dtype=torch.float32)\nval_numeric = torch.tensor(df[['Value','Unit']].iloc[val_idx].values, dtype=torch.float32)\n\n# -------- Combine Features --------\nX_train = torch.cat([\n    train_text_emb.cpu(), \n    train_img_emb.cpu(), \n    train_numeric.cpu()\n], dim=1).numpy()\n\nX_val = torch.cat([\n    val_text_emb.cpu(), \n    val_img_emb.cpu(), \n    val_numeric.cpu()\n], dim=1).numpy()\n\ny_train = df['log_price'].iloc[train_idx].values\ny_val = df['log_price'].iloc[val_idx].values\n\nprint(\"Feature shapes:\")\nprint(\"X_train:\", X_train.shape, \"X_val:\", X_val.shape)\nprint(\"y_train:\", y_train.shape, \"y_val:\", y_val.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T07:32:13.760915Z","iopub.execute_input":"2025-10-13T07:32:13.761132Z","iopub.status.idle":"2025-10-13T07:32:13.790713Z","shell.execute_reply.started":"2025-10-13T07:32:13.761115Z","shell.execute_reply":"2025-10-13T07:32:13.790108Z"}},"outputs":[{"name":"stdout","text":"Feature shapes:\nX_train: (12000, 1154) X_val: (3000, 1154)\ny_train: (12000,) y_val: (3000,)\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nimport torch\n\n# -------- LightGBM Dataset --------\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\n# -------- Parameters --------\nparams = {\n    'objective': 'regression',\n    'metric': 'mae',       # train with MAE\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': -1\n}\n\n# -------- Train --------\nmodel = lgb.train(params, lgb_train, num_boost_round=100, \n                  valid_sets=[lgb_train, lgb_val])\n\n# -------- Predict --------\ny_pred = model.predict(X_val)\n\n# -------- MAE --------\nmae = mean_absolute_error(y_val, y_pred)\nprint(\"Validation MAE:\", mae)\n\n# -------- SMAPE --------\ndef smape_loss(y_true, y_pred, eps=1e-7):\n    y_true_t = torch.tensor(y_true, dtype=torch.float32)\n    y_pred_t = torch.tensor(y_pred, dtype=torch.float32)\n    numerator = torch.abs(y_pred_t - y_true_t)\n    denominator = (torch.abs(y_true_t) + torch.abs(y_pred_t)) / 2 + eps\n    return torch.mean(numerator / denominator).item() * 100\n\nsmape = smape_loss(y_val, y_pred)\nprint(\"Validation SMAPE (%):\", smape)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T07:40:19.999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}